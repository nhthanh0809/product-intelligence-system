#!/usr/bin/env python3
"""
Script 01: MVP Extraction

Extract products from the source CSV file.
Validates required columns and removes invalid rows.

This script extracts raw data from the source CSV only.
No fields are generated - in enrich mode, additional fields
(brand, short_title, product_type, etc.) are generated by 02c_extract_with_llm.py.

Usage:
    python scripts/data-ingestion/01_extract_mvp.py
    python scripts/data-ingestion/01_extract_mvp.py --count 1000
"""

import json
import sys
import time
from datetime import datetime, timezone
from pathlib import Path

import click
import pandas as pd
import structlog

# Add src and scripts to path for imports
PROJECT_ROOT = Path(__file__).parent.parent.parent  # Product_intelligence_system/
sys.path.insert(0, str(PROJECT_ROOT / "services" / "data-pipeline"))  # for src.config
sys.path.insert(0, str(Path(__file__).parent))  # for config module

from src.config import get_settings
import config as cfg

# Configure logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ]
)
logger = structlog.get_logger()

# Required columns
REQUIRED_COLUMNS = [
    "asin",
    "title",
    "productURL",
]

# Default columns to extract (from source CSV)
# Can be overridden by config
DEFAULT_EXTRACT_COLUMNS = [
    "asin",
    "title",
    "productURL",
    "imgUrl",
    "price",
    "listPrice",
    "stars",
    "reviews",
    "categoryName",
    "isBestSeller",
    "boughtInLastMonth",
]


def validate_columns(df: pd.DataFrame) -> tuple[bool, list[str]]:
    """Validate that required columns exist."""
    missing = [col for col in REQUIRED_COLUMNS if col not in df.columns]
    return len(missing) == 0, missing


def clean_row(row: pd.Series) -> pd.Series:
    """Clean and validate a single row."""
    # Strip whitespace from string fields
    for col in ["asin", "title", "productURL"]:
        if pd.notna(row.get(col)):
            row[col] = str(row[col]).strip()
    return row


@click.command()
@click.option(
    "--input",
    "input_path",
    type=click.Path(exists=True),
    default=None,
    help="Input CSV file path",
)
@click.option(
    "--output",
    "output_path",
    type=click.Path(),
    default=None,
    help="Output CSV file path",
)
@click.option(
    "--count",
    "row_count",
    type=int,
    default=None,
    help="Number of products to extract",
)
@click.option(
    "--metrics-output",
    "metrics_path",
    type=click.Path(),
    default=None,
    help="Metrics output file path",
)
def main(input_path: str | None, output_path: str | None, row_count: int | None, metrics_path: str | None):
    """Extract MVP products from source CSV."""
    # Load settings
    settings = get_settings()
    product_count = cfg.get_count()
    pipeline_mode = cfg.get_mode()  # 'original' or 'enrich'

    # Use config file values, then CLI overrides
    # Note: {count} in paths is substituted with global.product_count
    input_path = input_path or str(cfg.get_path("01_extract_mvp", "input", str(settings.source_csv_path)))
    output_path = output_path or str(cfg.get_path("01_extract_mvp", "output", f"raw/mvp_{product_count}_products.csv"))
    row_count = row_count if row_count is not None else cfg.get_script("01_extract_mvp", "count", product_count)
    metrics_path = metrics_path or str(cfg.get_path("01_extract_mvp", "metrics", str(settings.metrics_dir / "01_extraction_metrics.json")))

    # Get columns to extract from config (or use defaults)
    script_config = cfg.get_script("01_extract_mvp") or {}
    extract_columns = script_config.get("extract_columns", DEFAULT_EXTRACT_COLUMNS)

    start_time = time.time()

    input_path = Path(input_path)
    output_path = Path(output_path)
    metrics_path = Path(metrics_path)

    # Ensure output directories exist
    output_path.parent.mkdir(parents=True, exist_ok=True)
    metrics_path.parent.mkdir(parents=True, exist_ok=True)

    logger.info(
        "starting_extraction",
        input_path=str(input_path),
        target_count=row_count,
        pipeline_mode=pipeline_mode,
    )

    # Initialize metrics
    metrics = {
        "stage": "extraction",
        "started_at": datetime.now(timezone.utc).isoformat(),
        "input_file": str(input_path),
        "output_file": str(output_path),
        "target_count": row_count,
        "pipeline_mode": pipeline_mode,
    }

    try:
        # Read CSV in chunks for memory efficiency
        logger.info("reading_csv", file=str(input_path))

        # First, check available columns
        sample_df = pd.read_csv(input_path, nrows=5)
        valid, missing = validate_columns(sample_df)

        if not valid:
            logger.error("missing_required_columns", missing=missing)
            metrics["status"] = "failed"
            metrics["error"] = f"Missing required columns: {missing}"
            with open(metrics_path, "w") as f:
                json.dump(metrics, f, indent=2)
            sys.exit(1)

        # Determine which columns to use (only extract columns that exist)
        available_columns = [col for col in extract_columns if col in sample_df.columns]
        logger.info("available_columns", columns=available_columns)

        # Read the required number of rows
        df = pd.read_csv(
            input_path,
            nrows=row_count + 10000,  # Read extra to account for filtering
            usecols=available_columns,
            dtype={
                "asin": str,
                "title": str,
                "productURL": str,
                "imgUrl": str,
                "categoryName": str,
            },
        )

        initial_count = len(df)
        logger.info("rows_read", count=initial_count)

        # Remove rows with NULL asin or productURL
        df = df.dropna(subset=["asin", "productURL"])
        after_null_removal = len(df)
        null_removed = initial_count - after_null_removal
        logger.info("null_rows_removed", count=null_removed)

        # Remove duplicates by asin
        df = df.drop_duplicates(subset=["asin"], keep="first")
        after_dedup = len(df)
        duplicates_removed = after_null_removal - after_dedup
        logger.info("duplicates_removed", count=duplicates_removed)

        # Take only the required count
        df = df.head(row_count)
        final_count = len(df)

        # Apply cleaning
        df = df.apply(clean_row, axis=1)

        # Validate schema
        valid_count = 0
        invalid_count = 0
        for _, row in df.iterrows():
            if pd.notna(row["asin"]) and pd.notna(row["title"]) and pd.notna(row["productURL"]):
                valid_count += 1
            else:
                invalid_count += 1

        # Check for productURL presence
        url_present = df["productURL"].notna().sum()

        # Save to output
        df.to_csv(output_path, index=False)
        logger.info("output_saved", path=str(output_path), count=final_count)

        end_time = time.time()
        processing_time = end_time - start_time

        # Update metrics
        metrics.update({
            "completed_at": datetime.now(timezone.utc).isoformat(),
            "status": "success",
            "processing_time_seconds": round(processing_time, 2),
            "metrics": {
                "initial_rows_read": initial_count,
                "null_rows_removed": null_removed,
                "duplicates_removed": duplicates_removed,
                "final_row_count": final_count,
                "valid_rows": valid_count,
                "invalid_rows": invalid_count,
                "extraction_success_rate": round(final_count / row_count * 100, 2),
                "schema_validation_rate": round(valid_count / final_count * 100, 2) if final_count > 0 else 0,
                "product_url_presence_rate": round(url_present / final_count * 100, 2) if final_count > 0 else 0,
                "duplicate_asin_rate": round(duplicates_removed / initial_count * 100, 4) if initial_count > 0 else 0,
            },
            "columns_extracted": available_columns,
        })

        # Save metrics
        with open(metrics_path, "w") as f:
            json.dump(metrics, f, indent=2)

        logger.info(
            "extraction_complete",
            final_count=final_count,
            processing_time=round(processing_time, 2),
            success_rate=metrics["metrics"]["extraction_success_rate"],
        )

        # Print summary
        print("\n" + "=" * 60)
        print("EXTRACTION COMPLETE")
        print("=" * 60)
        print(f"Pipeline mode:      {pipeline_mode}")
        print(f"Input file:         {input_path}")
        print(f"Output file:        {output_path}")
        print(f"Columns extracted:  {len(available_columns)}")
        print(f"Products extracted: {final_count:,}")
        print(f"Processing time:    {processing_time:.2f}s")
        print(f"Success rate:       {metrics['metrics']['extraction_success_rate']}%")
        print(f"Metrics saved to:   {metrics_path}")
        if pipeline_mode == "enrich":
            print("\nNext: Run 02a → 02b → 02c to add LLM-generated fields")
        print("=" * 60)

    except Exception as e:
        logger.error("extraction_failed", error=str(e))
        metrics["status"] = "failed"
        metrics["error"] = str(e)
        metrics["completed_at"] = datetime.now(timezone.utc).isoformat()
        with open(metrics_path, "w") as f:
            json.dump(metrics, f, indent=2)
        sys.exit(1)


if __name__ == "__main__":
    main()
