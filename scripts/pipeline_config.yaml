# Pipeline Configuration File
# ===========================
# This file configures the data pipeline scripts.
# All paths are relative to the data directory unless absolute paths are specified.
# Use {count} placeholder in filenames to auto-substitute with product_count value.
#
# TWO-PHASE WORKFLOW:
# ===================
# Phase 1 (Original Mode): Create parent nodes with basic product info
#   01_extract → 03_clean → 04_embed → 05_load
#   - Uses extract_columns from 01_extract_mvp
#   - Creates PARENT nodes only (no child nodes)
#   - Stores in Qdrant + PostgreSQL
#
# Phase 2 (Enrich Mode): Update parents + create child nodes
#   01_extract → 02a_download → 02b_html_to_md → 02c_extract_llm → 03_clean → 04_embed → 05_load
#   - Uses enriched data from 02c_extract_with_llm.py
#   - UPDATES existing parent nodes with basic_fields + parent_fields
#   - CREATES child nodes with child_*_fields (linked to existing parents)

# Global settings
global:
  # Base data directory (auto-detected if not specified)
  # data_dir: /path/to/data

  # Number of products to process (applies to all scripts if not overridden)
  # This value is used to substitute {count} in output filenames
  product_count: 50

  # Source CSV file (input for 01_extract_mvp.py)
  source_csv: archive/amz_ca_total_products_data_processed.csv

  # Pipeline execution mode:
  # - original: Phase 1 - Create parent nodes with basic product info
  #   01_extract → 03_clean → 04_embed → 05_load
  #   Creates PARENT nodes only, no child nodes
  # - enrich: Phase 2 - Update parents + create child nodes
  #   01_extract → 02a → 02b → 02c → 03_clean → 04_embed → 05_load
  #   Updates existing parents, creates child nodes linked to parents
  mode: original

  # Qdrant indexing strategy (mode-aware):
  # - parent_only: Create parent nodes only (for original mode)
  # - enrich_existing: Update existing parents + add child nodes (for enrich mode)
  # - full_replace: Delete and re-insert all points (clean slate)
  indexing_strategy: parent_only

# Script-specific configurations
scripts:
  # ============================================================================
  # 01_extract_mvp.py - Extract MVP products from source CSV
  # ============================================================================
  01_extract_mvp:
    # Input CSV file (relative to data_dir or absolute)
    input: archive/amz_ca_total_products_data_processed.csv
    # Output CSV file - {count} will be replaced with product_count
    output: raw/mvp_{count}_products.csv
    # Number of products to extract (null = use global.product_count)
    count: null
    # Metrics output file
    metrics: metrics/01_extraction_metrics.json

    # Columns to extract from source CSV (original mode uses only these)
    # In enrich mode, additional fields are generated by 02c_extract_with_llm.py
    extract_columns:
      - asin
      - title
      - productURL
      - imgUrl
      - price
      - listPrice
      - stars
      - reviews
      - categoryName
      - isBestSeller
      - boughtInLastMonth

  # ============================================================================
  # 02a_download_html.py - Download HTML from product URLs
  # ============================================================================
  02a_download_html:
    # Input CSV with product URLs
    input: raw/mvp_{count}_products.csv
    # Output directory for HTML files
    output_dir: scraped/html
    # Row range (1-indexed, inclusive)
    start_row: 1
    end_row: null  # null = process all
    count: null    # Alternative to end_row
    # Download settings
    concurrency: 4
    headless: true
    skip_existing: true
    # Metrics output
    metrics: metrics/02a_download_metrics.json

  # ============================================================================
  # 02b_html_to_markdown.py - Convert HTML to Markdown
  # ============================================================================
  02b_html_to_markdown:
    # Input directory with HTML files
    input_dir: scraped/html
    # Output directory for Markdown files
    output_dir: scraped/markdown
    # Conversion settings
    use_docling: false
    workers: 4
    skip_existing: true
    # Metrics output
    metrics: metrics/02b_markdown_metrics.json

  # ============================================================================
  # 02c_extract_with_llm.py - Extract from Markdown using LLM + GenAI Enrichment
  # ============================================================================
  02c_extract_with_llm:
    # Input directory with Markdown files
    input_dir: scraped/markdown
    # Output CSV with extracted data (includes {mode} for differentiation)
    output: scraped/mvp_{count}_{mode}_extracted.csv
    # Original CSV to merge with
    original_csv: raw/mvp_{count}_products.csv
    # LLM settings
    model: qwen3:8b
    ollama_url: http://192.168.80.54:11434
    concurrency: 4
    timeout: 180.0
    # Row range (1-indexed, inclusive) - for batch processing
    # Examples:
    #   start_row: 1, end_row: 1000    -> process files 1-1000
    #   start_row: 1001, end_row: 2000 -> process files 1001-2000
    #   start_row: 1, end_row: null    -> process all files
    start_row: 1
    end_row: null       # null = process all remaining files from start_row
    # Limit (alternative to end_row) - process N files starting from start_row
    limit: null
    # Metrics output
    metrics: metrics/02c_extraction_metrics.json

    # GenAI Enrichment Configuration (enrich mode only)
    # All LLM-generated fields are configured here.
    # Set field to true to generate with LLM, false to include as empty column.
    genai_enrichment:
      enabled: true  # Master switch for all GenAI fields

      # Basic LLM-extracted fields (from markdown content)
      basic_fields:
        brand: true                  # AI-extracted brand name
        short_title: true            # AI-generated concise title (max 6-8 words)
        product_type: true           # AI-extracted product type
        product_type_keywords: true  # AI-extracted product keywords
        availability: true           # AI-extracted availability status

      # Parent node quick-answer fields (stored in Qdrant for fast queries)
      parent_fields:
        genAI_summary: true              # 2-3 sentence product summary
        genAI_primary_function: true     # Main purpose in 1 sentence
        genAI_best_for: true             # Description of ideal user
        genAI_use_cases: true            # List of specific use cases
        genAI_target_audience: true      # Who should buy this
        genAI_key_capabilities: true     # Key product capabilities
        genAI_unique_selling_points: true # What makes it stand out
        genAI_value_score: true          # Value rating 1-10

      # Child node: Description fields
      child_description_fields:
        genAI_detailed_description: true # Expanded description
        genAI_how_it_works: true         # How the product works
        genAI_whats_included: true       # What's in the box
        genAI_materials: true            # Materials used

      # Child node: Features fields
      child_features_fields:
        genAI_features_detailed: true    # Detailed feature breakdown
        genAI_standout_features: true    # Top differentiating features
        genAI_technology_explained: true # Key technologies explained
        genAI_feature_comparison: true   # How it compares to alternatives

      # Child node: Specs fields
      child_specs_fields:
        genAI_specs_summary: true        # Human-readable specs summary
        genAI_specs_comparison_ready: true # Key specs for comparison
        genAI_specs_limitations: true    # Product limitations

      # Child node: Reviews/Sentiment fields
      child_reviews_fields:
        genAI_sentiment_score: true      # Sentiment score 0.0-1.0
        genAI_sentiment_label: true      # Sentiment label
        genAI_common_praises: true       # Common positive feedback
        genAI_common_complaints: true    # Common issues
        genAI_durability_feedback: true  # Durability assessment
        genAI_value_for_money_feedback: true # Value assessment

      # Child node: Use cases fields
      child_use_cases_fields:
        genAI_use_case_scenarios: true   # Specific use case scenarios
        genAI_ideal_user_profiles: true  # Ideal user profiles
        genAI_not_recommended_for: true  # Who should NOT buy this
        genAI_problems_solved: true      # Problems this product solves

  # ============================================================================
  # 03_clean_data.py - Clean and normalize scraped data
  # ============================================================================
  03_clean_data:
    # Input files (depends on pipeline mode):
    # - original mode: raw/mvp_{count}_products.csv
    # - enrich mode: scraped/mvp_{count}_{mode}_extracted.csv (or .json from 02c)
    input_original: raw/mvp_{count}_products.csv
    input_enrich: scraped/mvp_{count}_{mode}_extracted.csv
    # Output CSV with cleaned data (includes {mode} for differentiation)
    output: cleaned/mvp_{count}_{mode}_cleaned.csv
    # Number of products (null = all)
    count: null
    # Metrics output
    metrics: metrics/03_cleaning_metrics.json

    # Mode-specific behavior:
    # - original mode: Clean basic fields only, NO chunk building
    # - enrich mode: Clean enriched fields + build chunks for child nodes

    # Chunk sections to build (ONLY in enrich mode, for child nodes)
    # In original mode, these are SKIPPED
    chunk_sections:
      - description
      - features
      - specs
      - reviews
      - use_cases

    # Field name standardization (original CSV field names → DB field names)
    field_mapping:
      listPrice: list_price
      categoryName: category_level1
      isBestSeller: is_best_seller
      reviews: reviews_count
      imgUrl: img_url
      productURL: product_url
      boughtInLastMonth: bought_in_last_month

    # Scraped field mapping (02c output → standard names for chunk builders)
    # Only used in enrich mode
    scraped_field_mapping:
      scraped_description: product_description
      scraped_features: about_this_item
      scraped_technical_details: technical_details
      scraped_additional_info: additional_info
      scraped_top_reviews: top_reviews
      scraped_materials: materials
      scraped_whats_included: whats_included

  # ============================================================================
  # 04_generate_embeddings.py - Generate embeddings for products
  # ============================================================================
  04_generate_embeddings:
    # Input CSV (cleaned data, includes {mode} for differentiation)
    input: cleaned/mvp_{count}_{mode}_cleaned.csv
    # Output Parquet file with embeddings (includes {mode} for differentiation)
    output: embedded/mvp_{count}_{mode}_embedded.parquet
    # Number of products (null = all)
    count: null
    # Embedding settings
    ollama_url: http://localhost:11434
    model: bge-large
    batch_size: 100
    auto_pull: true
    # Checkpoint file for resume support (null = disabled)
    checkpoint: null
    # Metrics output
    metrics: metrics/04_embedding_metrics.json

    # Mode-specific behavior:
    # - original mode: Generate PARENT embeddings only (from extract_columns)
    # - enrich mode: Generate CHILD embeddings only (link to existing parents)

    # Child sections for enrich mode (creates child nodes linked to parents)
    # In original mode, these are SKIPPED - only parent embeddings generated
    child_sections:
      - description
      - features
      - specs
      - reviews
      - use_cases

    # Parent text fields for ORIGINAL mode embedding
    # Uses only fields from extract_columns in 01_extract_mvp
    parent_text_fields_original:
      - title           # Primary: product title (most important)
      - category_level1 # Secondary: helps with general queries

    # Parent text fields for ENRICH mode (update existing parent embedding)
    # Includes GenAI enrichment fields
    parent_text_fields_enrich:
      - title           # Primary: product title (most important)
      - brand           # From basic_fields
      - category_level1 # Tertiary: helps with general queries
      - genAI_summary   # From parent_fields
      - genAI_primary_function
      - genAI_best_for

  # ============================================================================
  # 05_load_stores.py - Load data into PostgreSQL, Qdrant, Elasticsearch
  # ============================================================================
  05_load_stores:
    # Input Parquet file (embedded data, includes {mode} for differentiation)
    input: embedded/mvp_{count}_{mode}_embedded.parquet
    # Number of products (null = all)
    count: null

    # Mode-specific behavior:
    # - original mode (parent_only): INSERT new parent nodes
    # - enrich mode (enrich_existing): UPDATE existing parents + INSERT child nodes

    # Indexing strategy (overrides global.indexing_strategy if set)
    # - parent_only: Insert parent nodes only (for original mode)
    # - enrich_existing: Update existing parents + add child nodes (for enrich mode)
    # - full_replace: Delete and re-insert all points
    indexing_strategy: null  # null = use global.indexing_strategy

    # PostgreSQL settings
    postgres_dsn: postgresql://pis_user:pis_password@localhost:5432/product_intelligence
    skip_postgres: false

    # Qdrant settings
    qdrant_host: localhost
    qdrant_port: 6333
    qdrant_collection: products
    skip_qdrant: false

    # Parent node fields for ORIGINAL mode (basic product info)
    # These are the fields from extract_columns in 01_extract_mvp
    qdrant_parent_fields_original:
      # Core identification
      - asin
      - title
      # Category hierarchy
      - category_level1
      # Display fields
      - price
      - list_price
      - stars
      - reviews_count
      - img_url
      - product_url
      - is_best_seller
      - bought_in_last_month

    # Parent node fields for ENRICH mode (update with enriched data)
    # These fields UPDATE existing parent nodes
    qdrant_parent_fields_enrich:
      # Basic fields (from 02c basic_fields)
      - brand
      - short_title
      - product_type
      - product_type_keywords
      - availability
      # Parent fields (from 02c parent_fields)
      - genAI_summary
      - genAI_primary_function
      - genAI_best_for
      - genAI_use_cases
      - genAI_target_audience
      - genAI_key_capabilities
      - genAI_unique_selling_points
      - genAI_value_score

    # Child node fields (section-specific content for enrich mode)
    qdrant_child_fields:
      - parent_asin
      - section
      - content_preview  # truncated content for preview

    # Child sections to create (5 sections per product, enrich mode only)
    child_sections:
      - description
      - features
      - specs
      - reviews
      - use_cases

    # Content preview max length (for lean payload)
    content_preview_max_length: 200

    # Elasticsearch settings
    elasticsearch_host: localhost
    elasticsearch_port: 9200
    elasticsearch_index: products
    skip_elasticsearch: false

    # Vector size (null = auto-detect from data)
    vector_size: null
    # Metrics output
    metrics: metrics/05_loading_metrics.json

  # ============================================================================
  # 04_generate_eval_data.py - Generate evaluation datasets
  # ============================================================================
  04_generate_eval_data:
    # Input CSV (always from 03_clean_data output, mode-aware)
    input: cleaned/mvp_{count}_{mode}_cleaned.csv
    # Output directory for evaluation datasets (files include {count}_{mode})
    output_dir: eval/datasets
    # Output filename patterns (include {count} and {mode} for matching with research-search-flows)
    output_files:
      level1: level1_embedding_{count}_{mode}.json
      level2: level2_indexing_{count}_{mode}.json
      level3: level3_retrieval_{count}_{mode}.json
      level4: level4_storage_{count}_{mode}.json
      level5: level5_agent_{count}_{mode}.json
      level6: level6_e2e_{count}_{mode}.json
    # Metrics output
    metrics: metrics/06_eval_data_generation_metrics.json

    # LLM settings for query generation (Levels 6, 5, 3)
    llm:
      enabled: false
      ollama_url: http://192.168.80.54:11434
      model: gpt-oss:120b
      timeout: 300
      max_retries: 3

    # Sample sizes per level (reduced for original mode, full for enrich mode)
    samples:
      level1: 100   # Embedding quality
      level2: 100   # Indexing correctness
      level3: 300   # Retrieval performance
      level4: 100   # Storage validation (skip GenAI tests in original mode)
      level5: 200   # Agent routing (limited in original mode)
      level6: 150   # E2E scenarios (simplified in original mode)

    # Original mode: only these fields available for eval
    original_mode_fields:
      - asin
      - title
      - price
      - list_price
      - stars
      - reviews_count
      - category_level1
      - is_best_seller
      - img_url
      - product_url
      - node_type
      - indexed_at

    # Fallback to template-based generation if LLM fails
    fallback_to_template: true

  # ============================================================================
  # run_pipeline.py - Pipeline orchestration
  # ============================================================================
  run_pipeline:
    # Stages to run (comma-separated or 'all')
    stages: all

    # Pipeline mode (overrides global.mode if set)
    # - original: Skip scraping (01_extract → 03_clean → 04_embed → 05_load → 06_eval)
    # - enrich: Full pipeline (01_extract → 02_scrape → 03_clean → 04_embed → 05_load → 06_eval)
    mode: null  # null = use global.mode

    # Skip stages if output exists
    skip_existing: false
    # Skip dependency checking
    skip_dependencies: false
    # Pipeline metrics output
    metrics: metrics/pipeline_metrics.json

  # ============================================================================
  # search_flows - Search flow experiments configuration
  # ============================================================================
  search_flows:
    # Evaluation data file (mode-aware, uses {count} and {mode} from global config)
    # Matches output from 06_generate_eval_data.py
    eval_data: eval/datasets/level3_retrieval_{count}_{mode}.json
    # Output report file (mode-aware)
    output_report: eval/search_experiments_{count}_{mode}_report.json

    # Database connections (can also use 05_load_stores settings)
    qdrant_host: localhost
    qdrant_port: 6333
    qdrant_collection: products

    elasticsearch_host: localhost
    elasticsearch_port: 9200
    elasticsearch_index: products

    # Embedding service
    ollama_url: http://localhost:8010
    embedding_model: bge-large

    # Reranker model
    reranker_model: qllama/bge-reranker-v2-m3

    # Search defaults
    default_limit: 10
    fetch_multiplier: 3  # Fetch N times limit for re-ranking

    # Search types to test (comma-separated or 'all')
    search_types: all

    # Verbose output
    verbose: false
